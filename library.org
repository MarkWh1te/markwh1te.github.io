#+TITLE: Python good library
#+BIND: org-html-validation-link nil

* Socket

* Python Basic Library 
python除了关键字（keywords）和内置的类型和函数（builtins）,更多的功能是通过libraries（即modules）来提供的。


常用的libraries（modules）如下：


 


）python运行时服务


 copy： copy模块提供了对复合（compound）对象（list，tuple，dict，custom class）进行浅拷贝和深拷贝的功能。


 pickle： pickle模块被用来序列化python的对象到bytes流，从而适合存储到文件，网络传输，或数据库存储。（pickle的过程也被称serializing,marshalling或者flattening，pickle同时可以用来将bytes流反序列化为python的对象）。


 sys：sys模块包含了跟python解析器和环境相关的变量和函数。


 其他： atexit，gc，inspect，marshal，traceback，types，warnings，weakref。





）数学


 decimal：python中的float使用双精度的二进制浮点编码来表示的，这种编码导致了小数不能被精确的表示，例如0.1实际上内存中为0.100000000000000001，还有3*0.1 == 0.3 为False. decimal就是为了解决类似的问题的，拥有更高的精确度，能表示更大范围的数字，更精确地四舍五入。


 math：math模块定义了标准的数学方法，例如cos(x),sin(x)等。


 random：random模块提供了各种方法用来产生随机数。


 其他：fractions，numbers。





）数据结构，算法和代码简化


 array： array代表数组，类似与list，与list不同的是只能存储相同类型的对象。


 bisect： bisect是一个有序的list，其中内部使用二分法（bitsection）来实现大部分操作。


 collections：collections模块包含了一些有用的容器的高性能实现，各种容器的抽象基类，和创建name-tuple对象的函数。例如包含了容器deque，defaultdict，namedtuple等。


 heapq：heapq是一个使用heap实现的带有优先级的queue。


 itertools：itertools包含了函数用来创建有效的iterators。所有的函数都返回iterators或者函数包含iterators（例如generators 和generators expression）。


 operator： operator提供了访问python内置的操作和解析器提供的特殊方法，例如 x+y 为 add（x，y），x+=y为iadd（x，y），a % b 为mod（a，b）等等。


 其他：abc，contextlib，functools。





) string 和 text 处理


codecs：codecs模块被用来处理不同的字符编码与unicode text io的转化。


 re：re模块用来对字符串进行正则表达式的匹配和替换。


 string：string模块包含大量有用的常量和函数用来处理字符串。也包含了新字符串格式的类。


 struct：struct模块被用来在python和二进制结构间实现转化。


 unicodedata：unicodedata模块提供访问unicode字符数据库




) python数据库访问


 关系型数据库拥有共同的规范Python Database API Specification V2.0，MySQL，Oracle等都实现了此规范，然后增加自己的扩展。


 sqlite3: sqlite3 模块提供了SQLite数据库访问的接口。SQLite数据库是以一个文件或内存的形式存在的自包含的关系型数据库。


 DBM-style 数据库模块：python提供了打了的modules来支持UNIX DBM-style数据库文件。dbm模块用来读取标准的UNIX-dbm数据库文件，gdbm用来读取GNU dbm数据库文件，dbhash用来读取Berkeley DB数据库文件。所有的这些模块提供了一个对象实现了基于字符串的持久化的字典，他与字典dict非常相似，但是他的keys和values都必须是字符串。


 shelve：shelve模块使用特殊的“shelf”对象来支持持久化对象。这个对象的行为与dict相似，但是所有的他存储的对象都使用基于hashtable的数据库（dbhash，dbm，gdbm）存储在硬盘。与dbm模块的区别是所存储的对象不仅是字符串，而且可以是任意的与pickle兼容的对象。





）文件和目录处理


 bz2：bz2模块用来处理以bzip2压缩算法压缩的文件。


 filecmp：filecmp模块提供了函数来比较文件和目录。


 fnmatch：fnmatch模块提供了使用UNIX shell-style的通配符来匹配文件名。这个模块只是用来匹配，使用glob可以获得匹配的文件列表。


 glob：glob模块返回了某个目录下与指定的UNIX shell通配符匹配的所有文件。


 gzip：gzip模块提供了类GzipFile，用来执行与GNUgzip程序兼容的文件的读写。


 shutil： shutil模块用来执行更高级别的文件操作，例如拷贝，删除，改名。shutil操作之针对一般的文件，不支持pipes，block devices等文件类型。


 tarfile： tarfile模块用来维护tar存档文件。tar没有压缩的功能。


 tempfile：tempfile模块用来产生临时文件和文件名。


 zipfile： zipfile模块用来处理zip格式的文件。


 zlib，zlib模块提供了对zlib库的压缩功能的访问。





）操作系统的服务


 cmmands： commands模块被用来执行简单的系统命令，命令以字符串的形式传入，且同时以字符串的形式返回命令的输出。但是此模块只在UNIX系统上可用。


 configParser，configParser模块用来读写windows的ini格式的配置文件。


 datetime，datetime模块提供了各种类型来表示和处理日期和时间。


 errno， 定义了所有的errorcode对应的符号名字。


 io，io模块实现了各种IO形式和内置的open()函数。


 logging， logging模块灵活方便地对应用程序记录events，errors，warnings，和debuging 信息。这些log信息可以被收集，过滤，写到文件或系统log，甚至通过网络发送到远程的机器上。


mmap，mmap模块提供了内存映射文件对象的支持，使用内存映射文件与使用一般的文件或byte字符串相似。


msvcrt，mscrt只可以在windows系统使用，用来访问Visual C运行时库的很多有用的功能。


optparse，optparse模块更高级别来处理UNIX style的命令行选项sys.argv。


 os，os模块对通用的操作系统服务提供了可移植的（portable）的接口。os可以认为是nt和posix的抽象。nt提供windows的服务接口，posix提供UNIX（linux，mac）的服务接口。


 os.path，os.path模块以可移植的方式来处理路径相关的操作。


 signal，signal模块用来实现信号（signal）处理，往往跟同步有关。


 subprocess，subprocess模块包含了函数和对象来统一创建新进程，控制新进程的输入输出流，处理进程的返回。


 time，time模块提供了各种时间相关的函数。常用的time.sleep().


 winreg, winreg模块用来操作windows注册表。


 其他：fcntl。





）线程和并行


 multiprocessing，multiprocessing模块提供通过subprocess来加载多个任务，通信，共享数据，执行各种同步操作。


 threading，threading模块提供了thread类很很多的同步方法来实现多线程编程。


 queue，queue模块实现了各种多生产者，多消费者队列，被用来实现多线程程序的信息安全交换。


 其他：Coroutines and Microthreading。





）网络编程和套接字（sockets）


 asynchat，asynchat模块通过封装asyncore来简化了应用程序的网络异步处理。


 ssl，ssl模块被用来使用secure sockets layer（SSL）包装socket对象，从而使得实现数据加密和终端认证。python使用openssl来实现此模块。


 socketserver，socketserver模块提供了类型简化了TCP，UDP和UNIX领域的socket server的实现。


 其他：asyncore，select。





0）internet应用程序编程


 ftplib，ftplib模块实现了ftp的client端协议。此模块很少使用，因为urllib提供了更高级的接口。


 http包，包含了http client和server的实现和cookies管理的模块。


 smtplib，smtplib包含了smtp client的底层接口，用来使用smtp协议发送邮件。


 urllib，urllib包提供了高级的接口来实现与http server，ftp server和本地文件交互的client。


 xmlrpc，xmlrpc模块被用类实现XML-RPC client。





1）web 编程


 cgi，cgi模块用来实现cgi脚本，cgi程序一般地被webserver执行，用来处理用户在form中的输入，或生成一些动态的内容。当与cgi脚本有管的request被提交，webserver将cgi作为子进程执行，cgi程序通过sys.stdin或环境变量来获得输入，通过sys.stdout来输出。


 webbrowser，webbrowser模块提供了平台独立的工具函数来使用web browser打开文档。


 其他：wsgiref/WSGI (Python Web Server Gateway Interface).





2) internet 数据处理和编码


 base64，base64模块提供了base64，base32，base16编码方式，用来实现二进制与文本间的编码和解码。base64通常用来对编码二进制数据，从而嵌入到邮件或http协议中。


 binascii，binascii模块提供了低级的接口来实现二进制和各种ASCII编码的转化。


 csv，csv模块用来读写comma-separated values（CSV）文件。


 email，email包提供了大量的函数和对象来使用MIME标准来表示，解析和维护email消息。


 hashlib，hashlib模块实现了各种secure hash和message digest algorithms，例如MD5和SHA1。


 htmlparser（html.parser），此模块定义了HTMLParser来解析HTML和XHTML文档。使用此类，需要定义自己的类且继承于HTMLParser。


 json，json模块被用类序列化或饭序列化Javascript object notation（JSON）对象。


 xml,xml包提供了各种处理xml的方法。

* Python Spark   
  经过多年来开拓性的工作，UC Berkeley AMP Lab开发了Spark。它使用分布式内存数据结构，提高了数据处理的速度，在大多数工作上优于Haddop。本文用一个真实的数据集，展示Spark的结构，以及基本的转换（transformations）与行动（actions）。如果你想尝试编写和运行自己的Spark代码，可以到Dataquest试试本教程的（英文）互动版本。

  弹性分布式数据集（RDD） 

  Spark的核心结构是RDD，全称“弹性分布式数据集”（resilient distributed dataset）。从名字即可看出，RDD是Spark里的数据集，分布于RAM或内存，或许多机器中。 一个RDD对象本质是多个元素的组合。它可以是包含多个元素（元组、列表、字典等）的列表。你可以把数据集载入为RDD，然后运行此RDD对象可用的方法（methods），就像Pandas里的数据框（DataFrames）。 

  PySpark 

  Spark是用Scala语言写成的，Scala把要编译的东西编译为Java虚拟机（JVM）的字节码（bytecode）。Spark的开源社区开发了一个叫PySpark的工具库。它允许使用者用Python处理RDD。这多亏了一个叫Py4J的库，它让Python可以使用JVM的对象（比如这里的RDD）。 
  开始操作之前，先把一个包含《每日秀》（the Daily Show）所有来宾的数据集载入为RDD。这里用的数据集是FiveThirtyEight’s dataset的tsv版本。TSV文件是由 “\t” 分隔的数据文件，不同于像CSV文件用逗号 “,” 分隔。 


  raw_data = sc.textFile("daily_show.tsv") 

  raw_data.take(5) 

  ['YEAR\tGoogleKnowlege_Occupation\tShow\tGroup\tRaw_Guest_List', 

  '1999\tactor\t1/11/99\tActing\tMichael J. Fox', 

  '1999\tComedian\t1/12/99\tComedy\tSandra Bernhard', 

  '1999\ttelevision actress\t1/13/99\tActing\tTracey Ullman', 

  '1999\tfilm actress\t1/14/99\tActing\tGillian Anderson'] 
  SparkContext 

  SparkContext 是管理Spark里的集群（cluster）和协调集群运行进程的对象。SparkContext与集群的manager相连。Manager负责管理运行具体运算的执行者。下面一幅图来自Spark官方文档，能更好地展示这个结构： 
  ﻿



  SparkContext对象通常以变量sc的形式被引用。运行： 


  raw_data = sc.textFile("daily_show.tsv") 
  把TSV数据集载入为 RDD对象raw_data 。这个RDD对象类似一个包含许多字符串对象（string objects）的列表，数据集中每一行是一个字符串对象。之后，使用 take() 方法打印出前五个元素： 


  raw_data.take(5) 
  take(n) 返回RDD的前n个元素。想了解更多RDD可用的方法，可查阅PySpark的官方文档。 

  惰性计算（Lazy Evaluation） 

  你可能会问：如果RDD与Python列表相似，那为什么不使用括号直接获取RDD里的元素？ 
  这是因为RDD对象分布于很多个部分，我们无法对其进行列表的标准操作，而且RDD本身就是为了处理分布式数据开发的。RDD抽象方式的优势是可以让Spark在本地计算机运行。在本地运行时，Spark把本地计算机的内存划分为很多部分，以模拟在许多机器上进行计算的情境，所以在本地运行时也无需改动代码。 
  Spark RDD的另一个优点是代码的惰性计算（lazily evaluate）。Spark把一个计算拖延到不得不运行的时候。在上面的代码中，直到运行 raw_data.take(5) ，Spark才把TSV文件载入RDD。当raw_data = sc.textFile(“dail_show.tsv”) 被调用时，创建了一个指向此文件的指针。但只有当raw_data.take(5) 需要此文件时，文件才真正被读取进raw_data。本教程以及未来的讲解中会出现更多惰性计算的例子。 

  流水线（Pipelines） 

  Spark大量借用了Hadoop的Map-Reduce模式，但许多地方与Hadoop不同。如果你有使用Hadoop和传统Map-Reduce的经验，Cloudera有一篇很棒的文章探讨这些差异。如果你从没使用过Map-Reduce 或 Hadoop也不用担心，本教程会介绍需要了解的概念。 
  使用Spark时，需要理解的核心概念是数据流水线（data pipelining）。Spark里的每个运算/计算本质是都是一系列步骤（step）。这些步骤能被连在一起，按顺序运行，形成一个流水线。流水线中的每个步骤返回一个Python值（例如Integer），一个Python数据结构（例如字典），或者一个RDD对象。首先，我们来看map() 函数。 
  Map() 
  map(f) 把函数f应用于RDD的每个元素。因为RDD是可迭代的（像多数Python对象一样），Spark每次迭代都运行f，之后返回一个新RDD。 
  为了便于理解，这里示范一个使用map 的例子。如果你观察仔细，就会发现 raw_data 目前的格式并不利于后续工作。现在每个元素都是一个字符串。为了便于管理，我们要把每个元素都转换成一个列表。Python的传统做法是： 

  使用for循环在集合中迭代 

  把每个字符串根据分隔符断开 

  把结果储存为一个列表 

  下面展示在Spark中使用map实现这个任务的方法。 
  在稍后的一段代码中，我们要： 

  调用RDD的map（）函数，把括号里的内容应用于数据集的每一行。 

  写一个lambda函数，根据分隔符”\t”把字符串分开，把结果存储为叫做daily_show的RDD。 

  在daily_show 上，调用RDD的take()函数，显示前五个元素（行）。 

  map(f) 函数是一个用于转换的步骤。需要提供给它一个命名过的函数或lambda函数。代码及输出如下： 


  daily_show = raw_data.map(lambda line: line.split('\t')) 

  daily_show.take(5) 

  [['YEAR', 'GoogleKnowlege_Occupation', 'Show', 'Group', 'Raw_Guest_List'], 

  ['1999', 'actor', '1/11/99', 'Acting', 'Michael J. Fox'], 

  ['1999', 'Comedian', '1/12/99', 'Comedy', 'Sandra Bernhard'], 

  ['1999', 'television actress', '1/13/99', 'Acting', 'Tracey Ullman'], 

  ['1999', 'film actress', '1/14/99', 'Acting', 'Gillian Anderson']] 
  Python与Scala，永远的好朋友 

  我们习惯了用Python写出任务的逻辑。PySpark众多的优点之一，是可以把逻辑和具体的数据转换分开。在上面的代码中，我们用Python写了一个lambda函数： 


  raw_data.map(lambda: line(line.split('\t'))) 
  而当这段代码运行于RDD时，又利用了Scala的优势。这就是PySpark的力量。尽管没有学习任何关于Scala的知识，我们还是利用了Spark的Scala结构在数据处理上的优异表现。更棒的是，当我们运行： 


  daily_show.take(5) 
  返回的结果还是对Python友好的格式。 
  转换与行动 
  Spark里有两类方法： 

  转换（Transformations） - map(), reduceByKey() 

  行动（Actions） - take(), reduce(), saveAsTextFile(), collect() 

  转换是惰性运算，总是返回对一个RDD对象的引用。不过，直到某个行动需要使用转换过的RDD，转换才会运行。任何返回RDD的函数都是转换，任何返回某个值的函数都是行动。在你实现本教程并尝试写PySpark代码的过程中，这些概念会变得更加清晰。 
  不可变 
  你可能会觉得奇怪：为什么不直接拆分每个字符串，而是要新建一个叫做daily_show的新对象？在Python中，可以直接逐个对集合里的元素进行修改，而不必返回或指派新对象。 
  RDD对象是不可变的。一旦对象被创建，它们的值就无法再变化。在Python里，列表和字典是可变的，这意味着我们可以改变这些对象的值，而元组是不可变的。在Python中修改一个元组对象，唯一方法就是创造一个包含所需改动的新元组。Spark利用RDD不可变的性质来提升速度，具体的原理超出本教程的讨论范围。 

  ReduceByKey() 

  我们想要对《每日秀》每年的来宾数目进行统计。在Python中，如果daily_show 是一个列表，其中包含多个列表，下面的一段代码可以实现我们的目的： 


  tally = dict() 

  for line in daily_show: 

  year = line[0] 

  if year in tally.keys(): 

  tally[year] = tally[year] + 1 

  else: 

  tally[year] = 1 
  tally 的每个键（key）会是唯一的，而值（value）是数据集中包含key的行数。 
  如果想用Spark获得相同结果，需要使用Map 步骤，接ReduceByKey步骤。 


  tally = daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y) 

  print(tally) 

  PythonRDD[156] at RDD at PythonRDD.scala:43 
  解释 

  你可能注意到了，打印tally 并没有像我们希望的那样返回统计数值。这是由于惰性计算的缘故。PySpark推迟map 和reduceByKey 的执行，直到需要使用它们（结果）的时候。在使用take() 预览tally 的前几个元素之前，先来过一遍上面的代码： 


  daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x+y) 
  在map 步骤，我们使用了一个lambda函数，用来创建一个元组其中包含： 

  键: x[0], 列表的第一个值 
  值: 1, 整数 

  这里的策略是创建一个元组，其中包含年份作键，取值为1。在运行map 之后，Spark 会在内存中保留一个类似下列形式的，由多个元组构成的列表： 


  ('YEAR', 1) 

  ('1991', 1) 

  ('1991', 1) 

  ('1991', 1) 

  ('1991', 1) 

  ... 
  而我们想把这些化简为： 

  ('YEAR', 1) 

  ('1991', 4) 

  ... 
  reduceByKey(f) 允许我们用函数f，将键相同的元组合并。 
  使用take 命令查看上面两个步骤的结果。take的作用是强迫惰性代码立即执行。由于 tally 是RDD，我们无法使用Python的len 函数来计算数目，而是要用RDD的 count() 函数。 


  tally.take(tally.count()) 

  [('YEAR', 1), 

  ('2013', 166), 

  ('2001', 157), 

  ('2004', 164), 

  ('2000', 169), 

  ('2015', 100), 

  ('2010', 165), 

  ('2006', 161), 

  ('2014', 163), 

  ('2003', 166), 

  ('2002', 159), 

  ('2011', 163), 

  ('2012', 164), 

  ('2008', 164), 

  ('2007', 141), 

  ('2005', 162), 

  ('1999', 166), 

  ('2009', 163)] 
  Filter 

  与Pandas不同的是，Spark无法识别首行是标题，也没有拿掉这些标题。我们需要想个办法从集合中去掉这个元素： 


  ('YEAR', 1) 
  你可能会试着从RDD里直接去掉这个元素，但请注意RDD是不可变的对象，一旦被创建就无法更改。去掉这个元组的唯一方法，就是创建一个不包含此元组的RDD对象。 
  Spark有一个filter(f) 函数。此函数允许我们根据现存的RDD创建一个新的RDD，新RDD中只包含符合要求的元素。定义一个只返回二元值True 或 False函数 f 。只有True 对应的项目会出现在最终的RDD中。更多有关filter函数的内容可见Spark官方文档。 


  def filter_year(line): 

  if line[0] == 'YEAR': 

  return False 

  else: 

  return Truefiltered_daily_show = daily_show.filter(lambda line: filter_year(line)) 
  大家一起来 

  为了展示Spark的强大，这一节示范如何把一系列数据转换连成一个流水线，并观察Spark在后台处理一切。Spark在编写时就意图为这个目的服务，而且为处理连续任务进行了高度优化。以前用 Hadoop连续处理大量任务非常耗时。这是因为实时产生的结果都需要被写入硬盘，而且Hadoop 根本没意识到完整流水线的重要性（如果你对此感到好奇，可以从这个网址了解更多http://qr.ae/RHWrT2）。 
  感谢Spark嚣张的内存使用方式（只把硬盘用作备份和特殊任务）以及建构合理的内核。与Hadoop相比，Spark可以大大节省周转时间。 
  在下面一段代码中，我们进行一系列操作：筛掉没有职业的来宾，把每个职业名称变为小写，统计各个职业，并输出统计结果的前五项。 


  filtered_daily_show.filter(lambda line: line[1] != '') \ 

  .map(lambda line: (line[1].lower(), 1)) \ 

  .reduceByKey(lambda x,y: x+y) \ 

  .take(5) 

  [('radio personality', 3), 

  ('television writer', 1), 

  ('american political figure', 2), 

  ('former united states secretary of state', 6), 

  ('mathematician', 1)] 

  后续 

  希望本教程激发了你对Spark的兴趣，并掌握了如何用PySpark编写我们熟悉的Python代码，同时利用分布式处理的优势。在涉及更大数据集的工作中，PySpark会大放光芒，因为它模糊了数据科学在“本地计算机”与“大型在线分布式计算（也被称作云）”中的界限。 
  如果你喜欢本教程，可以去Dataquest 阅读下一章节（英文版），下一章会进一步讲解Spark的转换与行动
